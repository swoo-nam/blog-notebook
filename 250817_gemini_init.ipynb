{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc6fbea9",
   "metadata": {},
   "source": [
    "# 라이브러리 Import\n",
    "- 공통적으로 사용하는 라이브러리를 불러옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4ac6bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273a7780",
   "metadata": {},
   "source": [
    "## 기존 VertexAI SDK 사용\n",
    "- google.oauth2의 service_account를 활용 \n",
    "- GCP의 Credential 파일을 통해 별도 로그인 없이 VertexAI 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73ff5a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.oauth2 import service_account\n",
    "import vertexai\n",
    "from vertexai.generative_models import (\n",
    "    GenerativeModel, \n",
    "    GenerationConfig,\n",
    "    Tool,\n",
    "    grounding,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1956ddc",
   "metadata": {},
   "source": [
    "- VERTEXAI_PROJECT_ID: Vertex AI 사용 권한이 있는 GCP 프로젝트의 ID\n",
    "- VERTEXAI_CREDENTIALS_PATH: Vertex AI에 접근 권한이 있는 서비스 계정의 “JSON 키 파일” 경로\n",
    "- **별도 Client를 받아 올 필요없이, init 한 번으로 호출 끝**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02ee0475",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "project_id = os.getenv(\"VERTEXAI_PROJECT_ID\")\n",
    "credential_path = os.getenv(\"VERTEXAI_CREDENTIALS_PATH\")\n",
    "credentials = service_account.Credentials.from_service_account_file(credential_path)\n",
    "\n",
    "vertexai.init(\n",
    "    project=project_id, \n",
    "    credentials=credentials\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f4d5a2",
   "metadata": {},
   "source": [
    "### Vertex AI 답변 생성 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2749e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vertexai_generate(\n",
    "    prompt,\n",
    "    model_name=\"gemini-1.5-flash\",\n",
    "    generation_config=GenerationConfig(),\n",
    "    tools=None\n",
    "):\n",
    "    model = GenerativeModel(\n",
    "        model_name=model_name,\n",
    "        tools=tools,\n",
    "        generation_config=generation_config,\n",
    "    )\n",
    "\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c70e58",
   "metadata": {},
   "source": [
    "- 기본 값(Default)으로 답변"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5861f7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result:  아직 2025년 한국 대통령이 누가 될지는 알 수 없습니다.  2022년 대선 이후로는  대한민국 대통령 선거가 5년마다 치러지기 때문에 다음 대선은 2027년에 있을 예정입니다.  따라서 2025년에는 현직 대통령이 계속해서 직무를 수행할 것입니다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"2025년 한국 대통령 누구야?\"\n",
    "\n",
    "result = vertexai_generate(prompt)\n",
    "print(\"result: \", result, flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd835e6",
   "metadata": {},
   "source": [
    "- 최신 모델(gemini 2.5 버전) 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01831673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result:  2025년 한국 대통령은 현재와 동일하게 **윤석열** 대통령입니다.\n",
      "\n",
      "윤석열 대통령은 2022년 5월 10일에 취임했으며, 대한민국 대통령의 임기는 5년 단임이므로 2027년 5월까지 재임하게 됩니다.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"gemini-2.5-flash\"\n",
    "prompt = \"2025년 한국 대통령 누구야?\"\n",
    "\n",
    "result = vertexai_generate(prompt, model_name)\n",
    "print(\"result: \", result, flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85806817",
   "metadata": {},
   "source": [
    "- Config 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b072bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result:  현재 대한민국 대통령은 **윤석열 대통령**입니다.\n",
      "\n",
      "대한민국 대통령의 임기는 5년 단임이며, 윤석열 대통령은 2022년 5월에 취임했습니다.\n",
      "\n",
      "따라서 **2025년에도 윤석열 대통령이 재임 중일 것입니다.** 다음 대통령 선거는 2027년에 치러질 예정입니다.\n"
     ]
    }
   ],
   "source": [
    "generation_config = GenerationConfig(\n",
    "    temperature=0.1,\n",
    "    top_p=0.95,\n",
    "    top_k=20,\n",
    "    candidate_count=1,\n",
    "    seed=5,\n",
    ")\n",
    "model_name = \"gemini-2.5-flash\"\n",
    "prompt = \"2025년 한국 대통령 누구야?\"\n",
    "\n",
    "result = vertexai_generate(prompt, model_name, generation_config)\n",
    "print(\"result: \", result, flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717551c8",
   "metadata": {},
   "source": [
    "- Tools 사용(Google 웹 검색)\n",
    "    - Vertex AI에서 2.0 이상 모델은 사용 가능하지만, **웹 검색은 1.5 버전까지만 지원**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66081c17",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgument",
     "evalue": "400 Unable to submit request because google_search_retrieval is not supported; please use google_search field instead. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/google/api_core/grpc_helpers.py:76\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallable_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/grpc/_interceptor.py:277\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    270\u001b[0m     request: Any,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    275\u001b[0m     compression: Optional[grpc\u001b[38;5;241m.\u001b[39mCompression] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    276\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m--> 277\u001b[0m     response, ignored_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/grpc/_interceptor.py:332\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable._with_call\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m    329\u001b[0m call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interceptor\u001b[38;5;241m.\u001b[39mintercept_unary_unary(\n\u001b[1;32m    330\u001b[0m     continuation, client_call_details, request\n\u001b[1;32m    331\u001b[0m )\n\u001b[0;32m--> 332\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, call\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/grpc/_channel.py:440\u001b[0m, in \u001b[0;36m_InactiveRpcError.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"See grpc.Future.result.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 440\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/grpc/_interceptor.py:315\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable._with_call.<locals>.continuation\u001b[0;34m(new_details, request)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 315\u001b[0m     response, call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_thunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_method\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwith_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_credentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_wait_for_ready\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_compression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _UnaryOutcome(response, call)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/grpc/_channel.py:1198\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.with_call\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m   1192\u001b[0m (\n\u001b[1;32m   1193\u001b[0m     state,\n\u001b[1;32m   1194\u001b[0m     call,\n\u001b[1;32m   1195\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blocking(\n\u001b[1;32m   1196\u001b[0m     request, timeout, metadata, credentials, wait_for_ready, compression\n\u001b[1;32m   1197\u001b[0m )\n\u001b[0;32m-> 1198\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_end_unary_response_blocking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/grpc/_channel.py:1006\u001b[0m, in \u001b[0;36m_end_unary_response_blocking\u001b[0;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[1;32m   1005\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1006\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _InactiveRpcError(state)\n",
      "\u001b[0;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.INVALID_ARGUMENT\n\tdetails = \"Unable to submit request because google_search_retrieval is not supported; please use google_search field instead. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:142.250.76.10:443 {grpc_message:\"Unable to submit request because google_search_retrieval is not supported; please use google_search field instead. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini\", grpc_status:3, created_time:\"2025-08-17T15:56:13.024487793+09:00\"}\"\n>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mInvalidArgument\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemini-2.5-flash\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     13\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2025년 한국 대통령 누구야?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 15\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult: \u001b[39m\u001b[38;5;124m\"\u001b[39m, result, flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[26], line 13\u001b[0m, in \u001b[0;36mgenerate_response\u001b[0;34m(prompt, model_name, generation_config, tools)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_response\u001b[39m(\n\u001b[1;32m      2\u001b[0m     prompt,\n\u001b[1;32m      3\u001b[0m     model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemini-1.5-flash\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m     generation_config\u001b[38;5;241m=\u001b[39mGenerationConfig(),\n\u001b[1;32m      5\u001b[0m     tools\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m      6\u001b[0m ):\n\u001b[1;32m      7\u001b[0m     model \u001b[38;5;241m=\u001b[39m GenerativeModel(\n\u001b[1;32m      8\u001b[0m         model_name\u001b[38;5;241m=\u001b[39mmodel_name,\n\u001b[1;32m      9\u001b[0m         tools\u001b[38;5;241m=\u001b[39mtools,\n\u001b[1;32m     10\u001b[0m         generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[1;32m     11\u001b[0m     )\n\u001b[0;32m---> 13\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mtext\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/vertexai/generative_models/_generative_models.py:695\u001b[0m, in \u001b[0;36m_GenerativeModel.generate_content\u001b[0;34m(self, contents, generation_config, safety_settings, tools, tool_config, labels, stream)\u001b[0m\n\u001b[1;32m    686\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_content_streaming(\n\u001b[1;32m    687\u001b[0m         contents\u001b[38;5;241m=\u001b[39mcontents,\n\u001b[1;32m    688\u001b[0m         generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    692\u001b[0m         labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[1;32m    693\u001b[0m     )\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 695\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43msafety_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafety_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtool_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtool_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/vertexai/generative_models/_generative_models.py:820\u001b[0m, in \u001b[0;36m_GenerativeModel._generate_content\u001b[0;34m(self, contents, generation_config, safety_settings, tools, tool_config, labels)\u001b[0m\n\u001b[1;32m    793\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generates content.\u001b[39;00m\n\u001b[1;32m    794\u001b[0m \n\u001b[1;32m    795\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[38;5;124;03m    A single GenerationResponse object\u001b[39;00m\n\u001b[1;32m    811\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    812\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_request(\n\u001b[1;32m    813\u001b[0m     contents\u001b[38;5;241m=\u001b[39mcontents,\n\u001b[1;32m    814\u001b[0m     generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    818\u001b[0m     labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[1;32m    819\u001b[0m )\n\u001b[0;32m--> 820\u001b[0m gapic_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prediction_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_response(gapic_response)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/google/cloud/aiplatform_v1/services/prediction_service/client.py:2275\u001b[0m, in \u001b[0;36mPredictionServiceClient.generate_content\u001b[0;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m   2272\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_universe_domain()\n\u001b[1;32m   2274\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[0;32m-> 2275\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2276\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2277\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2278\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2279\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2280\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2282\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[1;32m   2283\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/google/api_core/gapic_v1/method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compression\n\u001b[0;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/google/api_core/grpc_helpers.py:78\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m callable_(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[0;31mInvalidArgument\u001b[0m: 400 Unable to submit request because google_search_retrieval is not supported; please use google_search field instead. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini"
     ]
    }
   ],
   "source": [
    "google_search_retrieval = grounding.GoogleSearchRetrieval()\n",
    "tools = [Tool.from_google_search_retrieval(google_search_retrieval)]\n",
    "generation_config = GenerationConfig(\n",
    "    temperature=0.1,\n",
    "    top_p=0.95,\n",
    "    top_k=20,\n",
    "    candidate_count=1,\n",
    "    seed=5,\n",
    ")\n",
    "model_name = \"gemini-2.5-flash\"\n",
    "prompt = \"2025년 한국 대통령 누구야?\"\n",
    "\n",
    "result = vertexai_generate(prompt, model_name, generation_config, tools)\n",
    "print(\"result: \", result, flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b49e72",
   "metadata": {},
   "source": [
    "- 1.5 버전으로 모델 변경 후 재실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03002b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result:  2025년 대한민국 대통령은 이재명입니다.  2025년 6월 3일에 치러진 제21대 대통령 선거에서 당선되었습니다.  윤석열 전 대통령의 탄핵으로 인한 조기 대선이었습니다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "google_search_retrieval = grounding.GoogleSearchRetrieval()\n",
    "tools = [Tool.from_google_search_retrieval(google_search_retrieval)]\n",
    "generation_config = GenerationConfig(\n",
    "    temperature=0.1,\n",
    "    top_p=0.95,\n",
    "    top_k=20,\n",
    "    candidate_count=1,\n",
    "    seed=5,\n",
    ")\n",
    "model_name = \"gemini-1.5-flash\"\n",
    "prompt = \"2025년 한국 대통령 누구야?\"\n",
    "\n",
    "result = vertexai_generate(prompt, model_name, generation_config, tools)\n",
    "print(\"result: \", result, flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150a73d6",
   "metadata": {},
   "source": [
    "## 신규 GenAI SDK 사용\n",
    "- GenAI SDK는 GEMINI API를 사용하지만, 기존 VertexAI API를 필요에 따라 끌어 올 수 있다.\n",
    "- google.oauth2의 service_account 필요 없음\n",
    "- **Gemini API Key를 통해 Client 호출**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3be247df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai.types import (\n",
    "    Content,\n",
    "    Part,\n",
    "    GoogleSearch,\n",
    "    GenerateContentConfig,\n",
    "    Tool\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b525297",
   "metadata": {},
   "source": [
    "1) VertexAI API 사용 시\n",
    "    - VERTEXAI_USE: Vertex AI 사용 유무\n",
    "    - VERTEXAI_PROJECT_ID: Vertex AI 사용 권한이 있는 GCP 프로젝트의 ID\n",
    "    - VERTEXAI_LOCATION: Vertex AI에 접근 권한이 있는 GCP 프로젝트의 LOCATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8c882ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "# 반드시 해줘야 함\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = os.getenv(\"VERTEXAI_CREDENTIALS_PATH\")\n",
    "\n",
    "client = genai.Client(\n",
    "    vertexai=True,\n",
    "    project=\"gai-llm-poc\",\n",
    "    location=\"us-central1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35dcd182",
   "metadata": {},
   "source": [
    "2) 순수 GEMINI API 사용 시\n",
    "    - GENAI_API_KEY: GEMINI API 활성화 시 발급된 API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d100a479",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"GENAI_API_KEY\")\n",
    "\n",
    "client = genai.Client(\n",
    "    vertexai=False,\n",
    "    api_key=api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6575e9",
   "metadata": {},
   "source": [
    "### Gen AI 답변 생성 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "94b10ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genai_generate(\n",
    "    client,\n",
    "    prompt,\n",
    "    model_name=\"gemini-1.5-flash\",\n",
    "    generation_config=GenerateContentConfig(),\n",
    "):\n",
    "    contents = []\n",
    "    contents.append(\n",
    "        Content(\n",
    "            role=\"user\",\n",
    "            parts=[\n",
    "                Part.from_text(text=prompt)\n",
    "            ]\n",
    "        ),\n",
    "    )\n",
    "    response = client.models.generate_content(\n",
    "        model=model_name,\n",
    "        contents=contents,\n",
    "        config=generation_config,\n",
    "    )\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67840b0b",
   "metadata": {},
   "source": [
    "- 기본 값(Default)으로 답변"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a67b4dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result:  2025년 한국 대통령은 아직 결정되지 않았습니다.  2022년 대통령 선거에서 윤석열 후보가 당선되었고, 그의 임기는 2022년 5월 10일부터 2027년 5월 9일까지 입니다. 따라서 2025년에도 윤석열 대통령이 한국의 대통령일 것입니다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"2025년 한국 대통령 누구야?\"\n",
    "\n",
    "result = genai_generate(client, prompt)\n",
    "print(\"result: \", result, flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5f1307",
   "metadata": {},
   "source": [
    "- 최신 모델(gemini 2.5 버전) 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b130be79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result:  2025년 한국 대통령은 **윤석열 대통령**입니다.\n",
      "\n",
      "윤석열 대통령은 2022년 5월에 취임했으며, 대한민국 대통령의 임기는 5년 단임이기 때문에 2027년 5월까지가 그의 임기입니다. 따라서 2025년에도 윤석열 대통령이 재임 중입니다.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"gemini-2.5-flash\"\n",
    "prompt = \"2025년 한국 대통령 누구야?\"\n",
    "\n",
    "result = genai_generate(client, prompt, model_name)\n",
    "print(\"result: \", result, flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06859ab6",
   "metadata": {},
   "source": [
    "- Config 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4ef7c982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result:  2025년에도 **윤석열** 대통령입니다.\n",
      "\n",
      "대한민국 대통령의 임기는 5년 단임제이며, 윤석열 대통령은 2022년 5월 10일에 취임했으므로, 임기는 2027년 5월 9일까지입니다. 따라서 2025년에는 윤석열 대통령이 재임 중입니다.\n"
     ]
    }
   ],
   "source": [
    "generation_config = GenerateContentConfig(\n",
    "    temperature=0.1,\n",
    "    top_p=0.95,\n",
    "    top_k=20,\n",
    "    candidate_count=1,\n",
    "    seed=5,\n",
    ")\n",
    "model_name = \"gemini-2.5-flash\"\n",
    "prompt = \"2025년 한국 대통령 누구야?\"\n",
    "\n",
    "result = genai_generate(client, prompt, model_name, generation_config)\n",
    "print(\"result: \", result, flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c7ebaf",
   "metadata": {},
   "source": [
    "- Tools 사용(Google 웹 검색)\n",
    "    - **2.0 이상 모델 또한 웹 검색 사용 가능**\n",
    "    - Tools가 GenerateContentConfig 안에 포함되는 구조"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8a21b7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result:  2025년 대한민국의 대통령은 이재명입니다. 그는 2025년 6월 3일에 치러진 제21대 대통령 선거에서 당선되었으며, 임기는 2025년 6월 4일부터 2030년 6월 3일까지입니다. 이 선거는 윤석열 전 대통령의 탄핵으로 인해 조기에 실시되었습니다.\n"
     ]
    }
   ],
   "source": [
    "google_search = GoogleSearch()\n",
    "tools = [Tool(google_search=google_search)]\n",
    "generation_config = GenerateContentConfig(\n",
    "    temperature=0.1,\n",
    "    top_p=0.95,\n",
    "    top_k=20,\n",
    "    candidate_count=1,\n",
    "    seed=5,\n",
    "    tools=tools  # 안에 포함됨\n",
    ")\n",
    "model_name = \"gemini-2.5-flash\"\n",
    "prompt = \"2025년 한국 대통령 누구야?\"\n",
    "\n",
    "result = genai_generate(client, prompt, model_name, generation_config)\n",
    "print(\"result: \", result, flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
